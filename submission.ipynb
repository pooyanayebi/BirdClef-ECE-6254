{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        \nimport librosa\nimport torch\nfrom torch.utils.data import Dataset\nimport time \nimport os\nimport librosa\nimport numpy as np\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\n\ntest_audio_dir = '/kaggle/input/birdclef-2023/test_soundscapes/'\ntrain_audio_dir = '/kaggle/input/birdclef-2023/train_audio/'\ntest_audio_processed_dir = '/kaggle/working/test_audio_processed/'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-18T23:50:14.883621Z","iopub.execute_input":"2023-04-18T23:50:14.884440Z","iopub.status.idle":"2023-04-18T23:50:17.783183Z","shell.execute_reply.started":"2023-04-18T23:50:14.884399Z","shell.execute_reply":"2023-04-18T23:50:17.781837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, folder):\n        self.data = []\n        self.labels = []\n        for bird_folder in os.listdir(folder):\n            for file in os.listdir(os.path.join(folder, bird_folder)):\n                melspec = np.load(os.path.join(folder, bird_folder, file))\n                self.data.append(melspec)\n                self.labels.append(0)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        melspec = self.data[idx]\n        label = self.labels[idx]\n\n        # Zero-pad the mel-spectrogram to the target shape\n        target_shape = (128, 216)\n        padded_melspec = np.zeros(target_shape)\n        h, w = melspec.shape\n        padded_melspec[:h, :w] = melspec\n\n        return torch.tensor(padded_melspec, dtype=torch.float32).unsqueeze(0), torch.tensor(label)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T23:50:37.619855Z","iopub.execute_input":"2023-04-18T23:50:37.620567Z","iopub.status.idle":"2023-04-18T23:50:37.632064Z","shell.execute_reply.started":"2023-04-18T23:50:37.620506Z","shell.execute_reply":"2023-04-18T23:50:37.630729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BirdClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(BirdClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n        self.fc1 = nn.Linear(64 * 16 * 27, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.dropout = nn.Dropout(0.35)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.pool(self.relu(self.conv1(x)))\n        x = self.pool(self.relu(self.conv2(x)))\n        x = self.pool(self.relu(self.conv3(x)))\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\nnum_classes = len([i for i in os.listdir(train_audio_dir)])\nmodel = BirdClassifier(num_classes)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T23:50:44.078112Z","iopub.execute_input":"2023-04-18T23:50:44.078561Z","iopub.status.idle":"2023-04-18T23:50:44.210824Z","shell.execute_reply.started":"2023-04-18T23:50:44.078508Z","shell.execute_reply":"2023-04-18T23:50:44.209611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nstate_dict_name = \"bird_classifier035full.pth\"\nmodel_state_dict = torch.load(\"/kaggle/input/models/\" + state_dict_name, map_location=device)\nmodel.load_state_dict(model_state_dict)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T23:51:07.091818Z","iopub.execute_input":"2023-04-18T23:51:07.092200Z","iopub.status.idle":"2023-04-18T23:51:07.116991Z","shell.execute_reply.started":"2023-04-18T23:51:07.092169Z","shell.execute_reply":"2023-04-18T23:51:07.115624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_list = [f.split('.')[0] for f in sorted(os.listdir(test_audio_dir))]\npred = {'row_id': []}\nspecies_list = sorted(os.listdir(train_audio_dir))\nprint(len(species_list))\n\nfor i, species_code in enumerate(species_list):\n  pred[species_code] = []\n\nif not os.path.exists(test_audio_processed_dir):\n    os.makedirs(test_audio_processed_dir)\n\nfor f in file_list:\n  os.makedirs(os.path.join(test_audio_processed_dir, f), exist_ok=True)\n  y, sr = librosa.load(test_audio_dir + f + '.ogg')\n  num_clips = int(np.ceil(len(y) / (5 * sr)))\n  for i in range(num_clips):\n    start = i * 5 * sr\n    end = start + 5 * sr\n    y_clip = y[start:end]\n    if len(y_clip) == 5 * sr:\n      melspec = librosa.feature.melspectrogram(y = y_clip, sr=sr)\n      melspec = librosa.power_to_db(melspec)\n      output_file_path = os.path.join(test_audio_processed_dir, f, f\"{f.split('.')[0]}_clip{i}.npy\")\n      np.save(output_file_path, melspec)\n      row_id = f + '_' + str((i+1)*5)\n      pred['row_id'].append(row_id)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T23:51:09.905943Z","iopub.execute_input":"2023-04-18T23:51:09.906688Z","iopub.status.idle":"2023-04-18T23:51:28.135956Z","shell.execute_reply.started":"2023-04-18T23:51:09.906643Z","shell.execute_reply":"2023-04-18T23:51:28.133973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set = TestDataset(test_audio_processed_dir)\ntest_load = DataLoader(test_set, batch_size=1, shuffle=True)\n\nwith torch.no_grad():\n  for i, data in enumerate(test_load):\n    inputs, labels = data\n    inputs, labels = inputs.to(device), labels.to(device)\n    norm = torch.nn.Softmax(dim=1)\n    outputs = norm(model(inputs))\n    outputs = outputs[0].tolist()\n    for i, bird_id in enumerate(species_list):\n        prediction = outputs[i]\n        pred[bird_id].append(prediction)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T23:51:53.777879Z","iopub.execute_input":"2023-04-18T23:51:53.778608Z","iopub.status.idle":"2023-04-18T23:51:54.555443Z","shell.execute_reply.started":"2023-04-18T23:51:53.778566Z","shell.execute_reply":"2023-04-18T23:51:54.554141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(pred, columns = ['row_id'] + species_list)\nprint(submission)\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T23:51:58.341680Z","iopub.execute_input":"2023-04-18T23:51:58.342187Z","iopub.status.idle":"2023-04-18T23:51:58.469868Z","shell.execute_reply.started":"2023-04-18T23:51:58.342120Z","shell.execute_reply":"2023-04-18T23:51:58.468209Z"},"trusted":true},"execution_count":null,"outputs":[]}]}